{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Crawler\n",
    "\n",
    "## 1.0. Related example\n",
    "\n",
    "This code shows `wget`-like tool written in python. Run it from console (`python wget.py`), make it work. Check the code, reuse, and modify for your needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import argparse\n",
    "# import os\n",
    "# import re\n",
    "# import requests\n",
    "\n",
    "\n",
    "# def wget(url, filename):\n",
    "#     # allow redirects - in case file is relocated\n",
    "#     resp = requests.get(url, allow_redirects=True)\n",
    "#     # this can also be 2xx, but for simplicity now we stick to 200\n",
    "#     # you can also check for `resp.ok`\n",
    "#     if resp.status_code != 200:\n",
    "#         print(resp.status_code, resp.reason, 'for', url)\n",
    "#         return\n",
    "    \n",
    "#     # just to be cool and print something\n",
    "#     print(*[f\"{key}: {value}\" for key, value in resp.headers.items()], sep='\\n')\n",
    "#     print()\n",
    "    \n",
    "#     # try to extract filename from url\n",
    "#     if filename is None:\n",
    "#         # start with http*, ends if ? or # appears (or none of)\n",
    "#         m = re.search(\"^http.*/([^/\\?#]*)[\\?#]?\", url)\n",
    "#         filename = m.group(1)\n",
    "#         if not filename:\n",
    "#             raise NameError(f\"Filename neither given, nor found for {url}\")\n",
    "\n",
    "#     # what will you do in case 2 websites store file with the same name?\n",
    "#     if os.path.exists(filename):\n",
    "#         raise OSError(f\"File {filename} already exists\")\n",
    "    \n",
    "#     with open(filename, 'wb') as f:\n",
    "#         f.write(resp.content)\n",
    "#         print(f\"File saved as {filename}\")\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     parser = argparse.ArgumentParser(description='download file.')\n",
    "#     parser.add_argument(\"-O\", type=str, default=None, dest='filename', help=\"output file name. Default -- taken from resource\")\n",
    "#     parser.add_argument(\"url\", type=str, default=None, help=\"Provide URL here\")\n",
    "#     args = parser.parse_args()\n",
    "#     wget(args.url, args.filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.0.1. How to parse a page?\n",
    "\n",
    "If you build a crawler, you might follow one of the approaches:\n",
    "1. search for URLs in the page, assuming this is just a text.\n",
    "2. search for URLs in the places where URLs should appear: `<a href=..`, `<img src=...`, `<iframe src=...` and so on.\n",
    "\n",
    "To follow the first approach you can rely on some good regular expression. [Like this](https://stackoverflow.com/a/3809435).\n",
    "\n",
    "To follow the second approach just read one of these: [short answer](https://stackoverflow.com/questions/1080411/retrieve-links-from-web-page-using-python-and-beautifulsoup) or [exhaustive explanation](https://hackersandslackers.com/scraping-urls-with-beautifulsoup/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. [15] Download and persist #\n",
    "Please complete a code for `load()`, `download()` and `persist()` methods of `Document` class. What they do:\n",
    "- for a given URL `download()` method downloads binary data and stores in `self.content`. It returns `True` for success, else `False`.\n",
    "- `persist()` method saves `self.content` somewhere in file system. We do it to avoid multiple downloads (for caching in other words).\n",
    "- `load()` method loads data from hard drive. Returns `True` for success.\n",
    "\n",
    "Tests checks that your code somehow works.\n",
    "\n",
    "**NB Passing the test doesn't mean you correctly completed the task.** These are **criteria, which have to be fullfilled**:\n",
    "1. URL is a unique identifier (as it is a subset of URI). Thus, documents with different URLs should be stored in different files. Typical errors: documents from the same domain are overwritten to the same file, URLs with similar endings are downloaded to the same file, etc.\n",
    "2. The document can be not only a text file, but also a binary. Pay attention that if you download `mp3` file, it still can be played. Hint: don't hurry to convert everything to text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from urllib.parse import quote, urlparse\n",
    "from pathlib import Path\n",
    "from hashlib import sha512\n",
    "import httplib2\n",
    "\n",
    "def meta_redirect(url, content):\n",
    "    soup  = BeautifulSoup(content)\n",
    "\n",
    "    result=soup.find(\"meta\",attrs={\"http-equiv\":\"Refresh\"})\n",
    "    if result:\n",
    "        wait,text=result[\"content\"].split(\";\")\n",
    "        if text.strip().lower().startswith(\"url=\"):\n",
    "            re_url=text.split('=', maxsplit=1)[-1]\n",
    "            print(f'root url {url} re url {re_url}, net loc {urlparse(url).netloc}')\n",
    "            new_url = urllib.parse.urljoin('{uri.scheme}://{uri.netloc}/'.format(uri = urlparse(url)), re_url)\n",
    "            print(f'new url {new_url}')\n",
    "            return new_url\n",
    "    return None\n",
    "\n",
    "def get_content(url):\n",
    "    h=httplib2.Http(\".cache\")\n",
    "\n",
    "    resp, content = h.request(url,\"GET\")\n",
    "\n",
    "    # follow the chain of redirects\n",
    "    while new_url := meta_redirect(url, content):\n",
    "        resp, content = h.request(new_url,\"GET\") \n",
    "    return content\n",
    "\n",
    "class Document:\n",
    "    \n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        parsed = urlparse(url)\n",
    "        ext = ''.join(Path(parsed.path).suffixes)\n",
    "        Path('./Storage').mkdir(exist_ok=True)\n",
    "        self.file_name = f\"Storage/{sha512(url.encode('utf-8')).hexdigest()}{ext}\"\n",
    "\n",
    "    def get(self):\n",
    "        if not self.load():\n",
    "            if not self.download():\n",
    "                raise FileNotFoundError(self.url)\n",
    "            else:\n",
    "                self.persist()\n",
    "    \n",
    "    def download(self):\n",
    "        if not bool(urlparse(self.url).netloc):\n",
    "            return False #check if absolute path or not\n",
    "        try:\n",
    "            self.content = get_content(self.url)\n",
    "        except Exception as e:\n",
    "            print(e, \"on\", self.url)\n",
    "            return False\n",
    "        return True\n",
    "    \n",
    "    def persist(self):\n",
    "        #TODO write document content to hard drive\n",
    "\n",
    "        try:\n",
    "            with open(self.file_name, 'wb') as f:\n",
    "                f.write(self.content.encode())\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            return False\n",
    "        return True\n",
    "            \n",
    "    def load(self):\n",
    "        #TODO load content from hard drive, store it in self.content and return True in case of success\n",
    "        try:\n",
    "            with open(self.file_name, 'rb') as f:\n",
    "                self.content = f.read().decode()    \n",
    "        except:\n",
    "            return False\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1. Tests ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name 'BeautifulSoup' is not defined on http://sprotasov.ru/data/iu.txt\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "http://sprotasov.ru/data/iu.txt",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/user/Downloads/crawling/2022S-01 - Crawling.ipynb Cell 7'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/user/Downloads/crawling/2022S-01%20-%20Crawling.ipynb#ch0000006?line=0'>1</a>\u001b[0m doc \u001b[39m=\u001b[39m Document(\u001b[39m'\u001b[39m\u001b[39mhttp://sprotasov.ru/data/iu.txt\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/user/Downloads/crawling/2022S-01%20-%20Crawling.ipynb#ch0000006?line=2'>3</a>\u001b[0m doc\u001b[39m.\u001b[39;49mget()\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/user/Downloads/crawling/2022S-01%20-%20Crawling.ipynb#ch0000006?line=3'>4</a>\u001b[0m \u001b[39massert\u001b[39;00m doc\u001b[39m.\u001b[39mcontent, \u001b[39m\"\u001b[39m\u001b[39mDocument download failed\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/user/Downloads/crawling/2022S-01%20-%20Crawling.ipynb#ch0000006?line=4'>5</a>\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mCode snippets, demos and labs for the course\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mstr\u001b[39m(doc\u001b[39m.\u001b[39mcontent), \u001b[39m\"\u001b[39m\u001b[39mDocument content error\u001b[39m\u001b[39m\"\u001b[39m\n",
      "\u001b[1;32m/home/user/Downloads/crawling/2022S-01 - Crawling.ipynb Cell 5'\u001b[0m in \u001b[0;36mDocument.get\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/Downloads/crawling/2022S-01%20-%20Crawling.ipynb#ch0000004?line=40'>41</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mload():\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/Downloads/crawling/2022S-01%20-%20Crawling.ipynb#ch0000004?line=41'>42</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdownload():\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/user/Downloads/crawling/2022S-01%20-%20Crawling.ipynb#ch0000004?line=42'>43</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39murl)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/Downloads/crawling/2022S-01%20-%20Crawling.ipynb#ch0000004?line=43'>44</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/user/Downloads/crawling/2022S-01%20-%20Crawling.ipynb#ch0000004?line=44'>45</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpersist()\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: http://sprotasov.ru/data/iu.txt"
     ]
    }
   ],
   "source": [
    "doc = Document('http://sprotasov.ru/data/iu.txt')\n",
    "\n",
    "doc.get()\n",
    "assert doc.content, \"Document download failed\"\n",
    "assert \"Code snippets, demos and labs for the course\" in str(doc.content), \"Document content error\"\n",
    "\n",
    "doc.get()\n",
    "assert doc.load(), \"Load should return true for saved document\"\n",
    "assert \"Code snippets, demos and labs for the course\" in str(doc.content), \"Document load from disk error\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. [M][15] Account the caching policy\n",
    "\n",
    "Sometimes remote documents (especially when we speak about static content like `js` or `gif`) can swear that they will not change for some time. This is done by setting [Cache-Control response header](https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Cache-Control)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# requests.get('https://polyfill.io/v3/polyfill.min.js').headers['Cache-Control']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please study the documentation and implement a descendant to a `Document` class, which will refresh the document in case of expired cache even if the file is already on the hard drive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CachedDocument(Document):\n",
    "    \n",
    "#     # TODO your code here\n",
    "#     pass    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1. Tests\n",
    "\n",
    "Add logging in your code and show that your code behaves differently for documents with different caching policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import time\n",
    "\n",
    "# doc = CachedDocument('https://polyfill.io/v3/polyfill.min.js')\n",
    "# doc.get()\n",
    "# time.sleep(2)\n",
    "# doc.get()\n",
    "# time.sleep(2)\n",
    "# doc.get()\n",
    "\n",
    "# doc = CachedDocument('https://yandex.ru/')\n",
    "# doc.get()\n",
    "# time.sleep(2)\n",
    "# doc.get()\n",
    "# time.sleep(2)\n",
    "# doc.get()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. [10] Parse HTML ##\n",
    "`BeautifulSoap` library is a de facto standard to parse XML and HTML documents in python. Use it to complete `parse()` method that extracts document contents. You should initialize:\n",
    "- `self.anchors` list of tuples `('text', 'url')` met in a document. Be aware, there exist relative links (e.g. `../content/pic.jpg`). Use `urllib.parse.urljoin()` to fix this issue.\n",
    "- `self.images` list of images met in a document. Again, links can be relative to current page.\n",
    "- `self.text` should keep plain text of the document without scripts, tags, comments and so on. You can refer to [this stackoverflow answer](https://stackoverflow.com/a/1983219) for details.\n",
    "\n",
    "**NB All these 3 criteria must be fulfilled to get full point for the task.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from bs4.element import Comment\n",
    "import urllib.parse\n",
    "\n",
    "\n",
    "class HtmlDocument(Document):\n",
    "    \n",
    "    def tag_visible(self, element):\n",
    "        if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "            return False\n",
    "        if isinstance(element, Comment):\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def parse(self):\n",
    "        #TODO extract plain text, images and links from the document\n",
    "        soup = BeautifulSoup(self.content, 'html.parser')\n",
    "        self.anchors = []\n",
    "        self.images = []\n",
    "        self.text = \"\"\n",
    "\n",
    "        for link in soup.findAll(\"a\", text=True, href=True):\n",
    "            if link['href'] and link.text:\n",
    "                self.anchors.append((link.text, urllib.parse.urljoin(self.url, link['href'])))\n",
    "\n",
    "        for img in soup.findAll(\"img\"):\n",
    "            if (img_src:= img.attrs.get('src')) is not None:\n",
    "                self.images.append(urllib.parse.urljoin(self.url, img_src))\n",
    "\n",
    "        self.text = soup.findAll(text=True)\n",
    "        self.text = filter(self.tag_visible, self.text)\n",
    "        self.text = u' '.join(t.strip() for t in self.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1. Tests ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = HtmlDocument(\"http://sprotasov.ru\")\n",
    "doc.get()\n",
    "doc.parse()\n",
    "\n",
    "assert \"just few links\" in doc.text, \"Error parsing text\"\n",
    "assert \"http://sprotasov.ru/images/gb.svg\" in doc.images, \"Error parsing images\"\n",
    "assert any(p[1] == \"https://twitter.com/07C3\" for p in doc.anchors), \"Error parsing links\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. [10] Document analysis ##\n",
    "Complete the code for `HtmlDocumentTextData` class. Implement word and sentence splitting (use any method you can propose). \n",
    "\n",
    "**Criteria of success**: \n",
    "1. Your `get_word_stats()` method should return `Counter` object.\n",
    "2. Don't forget to lowercase your words for counting.\n",
    "3. Sentences should be obtained inside `<body>` tag only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "class HtmlDocumentTextData:\n",
    "    \n",
    "    def __init__(self, url):\n",
    "        self.doc = HtmlDocument(url)\n",
    "        self.doc.get()\n",
    "        self.doc.parse()\n",
    "\n",
    "    def tag_visible(self, element):\n",
    "        if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "            return False\n",
    "        if isinstance(element, Comment):\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "    def get_sentence(self):\n",
    "        #TODO extract plain text, images and links from the document\n",
    "        soup = BeautifulSoup(self.doc.content, 'html.parser')\n",
    "        result = []\n",
    "        if (body := soup.find('body')) is not None:\n",
    "            sentences = body.findAll(text=True)\n",
    "            sentences = filter(self.tag_visible, sentences)\n",
    "            for t in sentences:\n",
    "                result.extend(t.split('.'))\n",
    "        result = [t.strip() for t in result]\n",
    "        result = filter(''.__ne__, result)\n",
    "        return result\n",
    "    \n",
    "    def get_word_stats(self):\n",
    "        #TODO return Counter object of the document, containing mapping {`word` -> count_in_doc}\n",
    "        sentences = self.get_sentence()\n",
    "        words = []\n",
    "        for sentence in sentences:\n",
    "            words.extend(sentence.split(' '))\n",
    "        words = [word.strip().lower() for word in words]\n",
    "        words = filter(''.__ne__, words)\n",
    "        return Counter(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.1. Tests ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('и', 60), ('в', 32), ('иннополис', 21), ('2022', 15), ('по', 14), ('университет', 13), ('на', 13), ('области', 10), ('лаборатория', 10), ('для', 9)]\n"
     ]
    }
   ],
   "source": [
    "doc = HtmlDocumentTextData(\"https://innopolis.university/\")\n",
    "\n",
    "print(doc.get_word_stats().most_common(10))\n",
    "assert [x for x in doc.get_word_stats().most_common(10) if x[0] == 'иннополис'], 'иннополис should be among most common'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5. [M][35] Languages\n",
    "Maybe you heard, that there are multiple languages in the world. European languages, like Russian and English, use similar puctuation, but even in this family there is ¡Spanish!\n",
    "\n",
    "Other languages can use different punctiation rules, like **Arabic or [Thai](http://www.thai-language.com/ref/breaking-words)**.\n",
    "\n",
    "Your task is to support (at least) three languages (English, Arabic, and Thai) tokenization in your `HtmlDocumentTextData` class descendant.\n",
    "\n",
    "What should you do:\n",
    "1. Use any language dection techniques, e.g. [langdetect](https://pypi.org/project/langdetect/).\n",
    "2. Use language-specific tokenization tools, e.g. for [Thai](https://pythainlp.github.io/tutorials/notebooks/pythainlp_get_started.html#Tokenization-and-Segmentation) and [Arabic](https://github.com/CAMeL-Lab/camel_tools).\n",
    "3. Use these pages to test your code: [1](https://www.bangkokair.com/tha/baggage-allowance) and [2](https://alfajr-news.net/details/%D9%85%D8%B4%D8%B1%D9%88%D8%B9-%D8%AF%D9%8A%D9%85%D9%88%D9%82%D8%B1%D8%A7%D8%B7%D9%8A-%D9%81%D9%8A-%D8%A7%D9%84%D9%83%D9%88%D9%86%D8%BA%D8%B1%D8%B3-%D8%A7%D9%84%D8%A3%D9%85%D8%B1%D9%8A%D9%83%D9%8A-%D9%84%D9%85%D8%B9%D8%A7%D9%82%D8%A8%D8%A9-%D8%A8%D9%88%D8%AA%D9%8A%D9%86)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MultilingualHtmlDocumentTextData(HtmlDocumentTextData):\n",
    "    \n",
    "#     #TODO your code here\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5.1. Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc = MultilingualHtmlDocumentTextData(\"https://www.bangkokair.com/tha/baggage-allowance\")\n",
    "# print(doc.get_word_stats().most_common(10))\n",
    "\n",
    "# doc = MultilingualHtmlDocumentTextData(\"https://alfajr-news.net/details/%D9%85%D8%B4%D8%B1%D9%88%D8%B9-%D8%AF%D9%8A%D9%85%D9%88%D9%82%D8%B1%D8%A7%D8%B7%D9%8A-%D9%81%D9%8A-%D8%A7%D9%84%D9%83%D9%88%D9%86%D8%BA%D8%B1%D8%B3-%D8%A7%D9%84%D8%A3%D9%85%D8%B1%D9%8A%D9%83%D9%8A-%D9%84%D9%85%D8%B9%D8%A7%D9%82\")\n",
    "# print(doc.get_word_stats().most_common(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5. [15] Crawling ##\n",
    "\n",
    "Method `crawl_generator()` is given starting url (`source`) and max depth of search. It should return a **generator** of `HtmlDocumentTextData` objects (return a document as soon as it is downloaded and parsed). You can benefit from `yield obj_name` python construction. Use `HtmlDocumentTextData.anchors` field to go deeper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from queue import Queue\n",
    "\n",
    "class Crawler:\n",
    "    \n",
    "    def crawl_generator(self, source, depth=1):\n",
    "        #TODO return real crawling results. Don't forget to process failures\n",
    "        visited = set()\n",
    "        link_queue = Queue()\n",
    "        link_queue.put(source)\n",
    "        current_depth = 1\n",
    "        while current_depth <= depth:\n",
    "            next_link_queue = Queue()\n",
    "            print(f'Total number of links: {link_queue.qsize()}')\n",
    "            print(f\"Current Depth: {current_depth}\")\n",
    "            while not link_queue.empty():\n",
    "                cur_link = link_queue.get()\n",
    "                print(f'Links left: {link_queue.qsize()}')\n",
    "                visited.add(cur_link)\n",
    "                try:\n",
    "                    cur_doc = HtmlDocumentTextData(cur_link)\n",
    "                    yield cur_doc\n",
    "                    for link_tuple in cur_doc.doc.anchors:\n",
    "                        if link_tuple[1] not in visited:\n",
    "                            next_link_queue.put(link_tuple[1])\n",
    "                except FileNotFoundError:\n",
    "                    continue\n",
    "            link_queue = next_link_queue\n",
    "            current_depth+=1\n",
    "\n",
    "            # while not link_queue.empty():\n",
    "            #     for link_tuple in link_queue.get().doc.anchors:\n",
    "            #         if link_tuple[1] not in visited:\n",
    "            #             try:\n",
    "            #                 doc = HtmlDocumentTextData(link_tuple[1])\n",
    "            #             except FileNotFoundError:\n",
    "            #                 # print(f'Error with link {link_tuple[1]}')\n",
    "            #                 continue\n",
    "            #             yield doc\n",
    "            #             next_link_queue.put(doc)\n",
    "            #         visited.add(link_tuple[1]) \n",
    "            #     print(f'Links left: {link_queue.qsize()}')\n",
    "            # link_queue = next_link_queue\n",
    "            # current_depth+=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5. Tests ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of links: 1\n",
      "Current Depth: 1\n",
      "Links left: 0\n",
      "https://innopolis.university/en/\n",
      "368 distinct word(s) so far\n",
      "Total number of links: 51\n",
      "Current Depth: 2\n",
      "Links left: 50\n",
      "https://apply.innopolis.university/en\n",
      "1254 distinct word(s) so far\n",
      "Links left: 49\n",
      "https://corporate.innopolis.university/en\n",
      "1434 distinct word(s) so far\n",
      "Links left: 48\n",
      "https://media.innopolis.university/en\n",
      "1496 distinct word(s) so far\n",
      "Links left: 47\n",
      "https://innopolis.university/lk/\n",
      "1867 distinct word(s) so far\n",
      "Links left: 46\n",
      "https://innopolis.university/en/team-structure/\n",
      "1874 distinct word(s) so far\n",
      "Links left: 45\n",
      "https://innopolis.university/en/team-structure/education-academics/\n",
      "1880 distinct word(s) so far\n",
      "Links left: 44\n",
      "https://innopolis.university/en/team-structure/techcenters/\n",
      "1884 distinct word(s) so far\n",
      "Links left: 43\n",
      "https://innopolis.university/en/faculty/\n",
      "3054 distinct word(s) so far\n",
      "Links left: 42\n",
      "https://career.innopolis.university/en/job/\n",
      "3590 distinct word(s) so far\n",
      "Links left: 41\n",
      "https://apply.innopolis.university/en/bachelor/CE/\n",
      "3672 distinct word(s) so far\n",
      "Links left: 40\n",
      "https://apply.innopolis.university/en/bachelor/DS-AI/\n",
      "3705 distinct word(s) so far\n",
      "Links left: 39\n",
      "https://apply.innopolis.university/en/master/datascience/\n",
      "3835 distinct word(s) so far\n",
      "Links left: 38\n",
      "https://apply.innopolis.university/en/master/securityandnetworkengineering/\n",
      "3918 distinct word(s) so far\n",
      "Links left: 37\n",
      "https://apply.innopolis.university/en/master/development/\n",
      "3982 distinct word(s) so far\n",
      "Links left: 36\n",
      "https://apply.innopolis.university/en/master/robotics/\n",
      "4069 distinct word(s) so far\n",
      "Links left: 35\n",
      "https://apply.innopolis.university/en/master/technological-entrepreneurship/\n",
      "4299 distinct word(s) so far\n",
      "Links left: 34\n",
      "https://innopolis.university/en/incomingstudents/\n",
      "4372 distinct word(s) so far\n",
      "Links left: 33\n",
      "https://innopolis.university/en/outgoingstudents/\n",
      "4722 distinct word(s) so far\n",
      "Links left: 32\n",
      "https://innopolis.university/en/lab-operating-systems/\n",
      "4763 distinct word(s) so far\n",
      "Links left: 31\n",
      "https://innopolis.university/en/lab-software-service-engineering/\n",
      "4869 distinct word(s) so far\n",
      "Links left: 30\n",
      "https://innopolis.university/en/lab-industrializing-software/\n",
      "4914 distinct word(s) so far\n",
      "Links left: 29\n",
      "https://innopolis.university/en/lab-bioinformatics/\n",
      "4962 distinct word(s) so far\n",
      "Links left: 28\n",
      "https://innopolis.university/en/lab-game-development/\n",
      "5028 distinct word(s) so far\n",
      "Links left: 27\n",
      "https://innopolis.university/en/lab-oil-gas/\n",
      "5077 distinct word(s) so far\n",
      "Links left: 26\n",
      "https://innopolis.university/en/mlkr/\n",
      "5253 distinct word(s) so far\n",
      "Links left: 25\n",
      "https://innopolis.university/en/lab-cyberphysical-systems/\n",
      "5296 distinct word(s) so far\n",
      "Links left: 24\n",
      "https://innopolis.university/en/lab-networks-blockchain/\n",
      "5359 distinct word(s) so far\n",
      "Links left: 23\n",
      "https://innopolis.university/en/lab-robotics/\n",
      "5464 distinct word(s) so far\n",
      "Links left: 22\n",
      "https://innopolis.university/lk/\n",
      "5464 distinct word(s) so far\n",
      "Links left: 21\n",
      "https://university.innopolis.ru/en/about/\n",
      "5555 distinct word(s) so far\n",
      "Links left: 20\n",
      "https://apply.innopolis.university/en/bachelor/\n",
      "5575 distinct word(s) so far\n",
      "Links left: 19\n",
      "https://apply.innopolis.university/en/master/\n",
      "5582 distinct word(s) so far\n",
      "Links left: 18\n",
      "https://apply.innopolis.university/en/postgraduate-study/\n",
      "5657 distinct word(s) so far\n",
      "Links left: 17\n",
      "http://www.campuslife.innopolis.ru\n",
      "5775 distinct word(s) so far\n",
      "Links left: 16\n",
      "https://innopolis.university/en/international-relations-office/\n",
      "5796 distinct word(s) so far\n",
      "Links left: 15\n",
      "https://innopolis.university/en/research/\n",
      "5834 distinct word(s) so far\n",
      "Links left: 14\n",
      "https://www.facebook.com/InnopolisU\n",
      "5834 distinct word(s) so far\n",
      "Links left: 13\n",
      "https://vk.com/innopolisu\n",
      "6172 distinct word(s) so far\n",
      "Links left: 12\n",
      "https://www.youtube.com/user/InnopolisU\n",
      "6187 distinct word(s) so far\n",
      "Links left: 11\n",
      "https://www.instagram.com/innopolisu/\n",
      "6187 distinct word(s) so far\n",
      "Links left: 10\n",
      "https://apply.innopolis.ru/en/index.php\n",
      "6588 distinct word(s) so far\n",
      "Links left: 9\n",
      "https://university.innopolis.ru/en/cooperation/\n",
      "6591 distinct word(s) so far\n",
      "Links left: 8\n",
      "https://university.innopolis.ru/en/about/\n",
      "6591 distinct word(s) so far\n",
      "Links left: 7\n",
      "https://career.innopolis.university/en/\n",
      "6809 distinct word(s) so far\n",
      "Links left: 6\n",
      "https://panoroo.com/virtual-tours/NvQZM6B2\n",
      "6809 distinct word(s) so far\n",
      "Links left: 5\n",
      "https://innopolis.university/en/contacts/\n",
      "6817 distinct word(s) so far\n",
      "Links left: 4\n",
      "https://media.innopolis.university/en/news/\n",
      "6817 distinct word(s) so far\n",
      "Links left: 3\n",
      "https://media.innopolis.university/en/events/\n",
      "6819 distinct word(s) so far\n",
      "Links left: 2\n",
      "http://www.minsvyaz.ru/en/\n",
      "6913 distinct word(s) so far\n",
      "Links left: 1\n",
      "[WinError 10060] A connection attempt failed because the connected party did not properly respond after a period of time, or established connection failed because connected host has failed to respond on http://минобрнауки.рф/\n",
      "Links left: 0\n",
      "https://innopolis.university/public/files/Consent_to_the_processing_of_PD_for_UI.pdf\n",
      "Skipping https://innopolis.university/public/files/Consent_to_the_processing_of_PD_for_UI.pdf\n",
      "Done\n",
      "[('and', 2511), ('of', 2392), ('the', 1985), ('in', 1281), ('to', 893), ('university', 719), ('for', 644), ('a', 609), ('at', 400), ('research', 393), ('lab', 385), ('innopolis', 362), ('software', 360), ('science', 346), ('development', 333), ('with', 327), ('you', 322), ('it', 310), ('is', 294), ('data', 289)]\n"
     ]
    }
   ],
   "source": [
    "crawler = Crawler()\n",
    "counter = Counter()\n",
    "\n",
    "for c in crawler.crawl_generator(\"https://innopolis.university/en/\", 2):\n",
    "    print(c.doc.url)\n",
    "    if c.doc.url[-4:] in ('.pdf', '.mp3', '.avi', '.mp4', '.txt'):\n",
    "        print(\"Skipping\", c.doc.url)\n",
    "        continue\n",
    "    counter.update(c.get_word_stats())\n",
    "    print(len(counter), \"distinct word(s) so far\")\n",
    "    \n",
    "print(\"Done\")\n",
    "\n",
    "print(counter.most_common(20))\n",
    "assert [x for x in counter.most_common(20) if x[0] == 'innopolis'], 'innopolis sould be among most common'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
